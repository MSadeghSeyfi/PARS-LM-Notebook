import streamlit as st
from langchain_community.document_loaders import PyPDFium2Loader
from pathlib import Path
from langchain.text_splitter import RecursiveCharacterTextSplitter
from typing import List
import re

class App:
    def __init__(self):
        self.translations = {
            'fa' : {
                'change_language': 'ØªØºÛŒÛŒØ± Ø²Ø¨Ø§Ù†',
                'current_language': 'Ø²Ø¨Ø§Ù† ÙØ¹Ù„ÛŒ: ÙØ§Ø±Ø³ÛŒ',
                'app_title': 'Ú©Ø§Ø± Ø¨Ø§ PDF',
                'placeholder_genre': 'Ù„Ø·ÙØ§ Ø¯Ø± Ø¨Ø§Ø±Ù‡ ÛŒ PDF Ø³ÙˆØ§Ù„ÛŒ Ø¨Ù¾Ø±Ø³ÛŒØ¯ .',
                'select_file' : 'ÙŠÚ© ÙØ§ÙŠÙ„ PDf Ø¨Ø±Ú¯Ø²ÙŠÙ†ÙŠØ¯ .'
            },
            'en' : {
                'change_language': 'Change Language',
                'current_language': 'Current Language: English',
                'app_title': 'work with PDF',
                'placeholder_genre': 'Please ask a question about pdf .',
                'select_file' : 'Please select a PDF file .'
            }
        }

        if 'language' not in st.session_state:
            st.session_state.language = 'fa'

        if st.button(self.translations[st.session_state.language]['change_language']):
            st.session_state.language = 'en' if st.session_state.language == 'fa' else "fa"

        self.language = st.session_state.language
        self.text_align, self.direction = self.get_text_alignment()
        self.inject_css()
        self.model = None

    def get_text_alignment(self):
            if self.language == 'fa':
                return 'right', 'rtl'
            else:
                return 'left', 'ltr'    
        
    def inject_css(self):
            st.markdown(f"""
                <style>
                        
                    @import url('https://fonts.googleapis.com/css2?family=Vazirmatn:wght@400;500&display=swap');
                    
                    * {{
                        font-family: 'Vazirmatn', sans-serif;
                        direction: {self.direction};
                        text-align: {self.text_align};
                    }}

                </style>
            """, unsafe_allow_html=True)  

    def minimal_persian_fix(self, text):
                # Ascending
                # ØªØµØ­ÛŒØ­ Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§ÛŒ Ø§Ø´ØªØ¨Ø§Ù‡
                char_fixes = {
                    'Í·': 'Ú©',  # Ú©Ø§Ø±Ø§Ú©ØªØ± Ø§Ø´ØªØ¨Ø§Ù‡ Ø¨Ù‡ Ø¬Ø§ÛŒ Ú©
                    'Íº': 'Ú©',  # Ú©Ø§Ø±Ø§Ú©ØªØ± Ø§Ø´ØªØ¨Ø§Ù‡ Ø¯ÛŒÚ¯Ø±
                    'Í½': 'Ú¯',  # Ú©Ø§Ø±Ø§Ú©ØªØ± Ø§Ø´ØªØ¨Ø§Ù‡ Ø¨Ù‡ Ø¬Ø§ÛŒ Ú¯
                    'Ø¤': 'Ùˆ',  # Ùˆ Ù‡Ù…Ø²Ù‡â€ŒØ¯Ø§Ø± Ø¹Ø±Ø¨ÛŒ Ø¨Ù‡ ÙØ§Ø±Ø³ÛŒ
                    'Ù‰': 'ÛŒ',  # ÛŒ Ø¹Ø±Ø¨ÛŒ Ø¨Ù‡ ÙØ§Ø±Ø³ÛŒ
                    'Ø©': 'Ù‡',  # ØªÙ‡ Ù…Ø±Ø¨ÙˆØ·Ù‡ Ø¹Ø±Ø¨ÛŒ Ø¨Ù‡ ÙØ§Ø±Ø³ÛŒ
                    'Ùƒ': 'Ú©',  # Ú©Ø§Ù Ø¹Ø±Ø¨ÛŒ Ø¨Ù‡ ÙØ§Ø±Ø³ÛŒ
                    'ÙŠ': 'ÛŒ',  # ÛŒØ§ Ø¹Ø±Ø¨ÛŒ Ø¨Ù‡ ÙØ§Ø±Ø³ÛŒ
                }

                # Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ†ÛŒ Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§
                for wrong, correct in char_fixes.items():
                    text = text.replace(wrong, correct)

                # ØªØµØ­ÛŒØ­ Ù…Ø´Ú©Ù„ Ø®Ø§Øµ chr(876) + ÙØ§ØµÙ„Ù‡
                text = text.replace(chr(876) + ' ', 'ÛŒ ')
                text = text.replace(chr(876) + chr(13), 'ÛŒ ')
                text = text.replace(chr(876), 'ÛŒ')  # Ø¨Ø±Ø§ÛŒ Ù…ÙˆØ§Ø±Ø¯ Ø¯ÛŒÚ¯Ø± chr(876)

                return text

    def extract_text_from_pdf(self, pdf_path: str) -> str:
            try:
                # Load the PDF using PyPDFium2Loader
                loader = PyPDFium2Loader(pdf_path)
                documents = loader.load()
                text = ""

                # Iterate through all documents (each represents a page)
                for doc in documents:
                    original_content = doc.page_content
                    corrected_content = self.minimal_persian_fix(original_content)
                    text += corrected_content + "\n"  # Add newline between pages
                
                return text
            except Exception as e:
                return f"Error extracting text: {str(e)}"
        

    def advanced_persian_chunking(self, text: str) -> List[str]:
        """
        Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡ ØªÙ‚Ø³ÛŒÙ… Ù…ØªÙ† ÙØ§Ø±Ø³ÛŒ Ø¨Ù‡ Ú†Ø§Ù†Ú©â€ŒÙ‡Ø§ÛŒ Ø¨Ù‡ÛŒÙ†Ù‡
        """
        
        # 1. Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ø§ÙˆÙ„ÛŒÙ‡ Ù…ØªÙ†
        def preprocess_text(text: str) -> str:
            # Ø­Ø°Ù ÙØ§ØµÙ„Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø¶Ø§ÙÛŒ
            text = re.sub(r'\s+', ' ', text)
            # ØªÙ†Ø¸ÛŒÙ… ÙØ§ØµÙ„Ù‡â€ŒÙ‡Ø§ Ù‚Ø¨Ù„ Ùˆ Ø¨Ø¹Ø¯ Ø¹Ù„Ø§Ø¦Ù… Ù†Ú¯Ø§Ø±Ø´ÛŒ ÙØ§Ø±Ø³ÛŒ
            text = re.sub(r'\s*([.!?ØŸ:Ø›ØŒ])\s*', r'\1 ', text)
            # Ø­Ø°Ù Ø®Ø·ÙˆØ· Ø®Ø§Ù„ÛŒ Ø§Ø¶Ø§ÙÛŒ
            text = re.sub(r'\n\s*\n', '\n\n', text)
            return text.strip()
        
        # 2. ØªØ¹Ø±ÛŒÙ Ø¬Ø¯Ø§Ú©Ù†Ù†Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ù‡ÛŒÙ†Ù‡ Ø¨Ø±Ø§ÛŒ ÙØ§Ø±Ø³ÛŒ
        persian_separators = [
            "\n\n\n",          # Ù¾Ø§Ø±Ø§Ú¯Ø±Ø§Ùâ€ŒÙ‡Ø§ÛŒ Ø¬Ø¯Ø§
            "\n\n",            # Ù¾Ø§Ø±Ø§Ú¯Ø±Ø§Ùâ€ŒÙ‡Ø§
            "\n",              # Ø®Ø·ÙˆØ· Ø¬Ø¯ÛŒØ¯
            "ØŸ",               # Ø¹Ù„Ø§Ù…Øª Ø³ÙˆØ§Ù„ ÙØ§Ø±Ø³ÛŒ
            "!",               # Ø¹Ù„Ø§Ù…Øª ØªØ¹Ø¬Ø¨
            ".",               # Ù†Ù‚Ø·Ù‡
            "Ø›",               # Ù†Ù‚Ø·Ù‡ ÙˆÛŒØ±Ú¯ÙˆÙ„ ÙØ§Ø±Ø³ÛŒ
            ":",               # Ø¯Ùˆ Ù†Ù‚Ø·Ù‡
            "ØŒ",               # ÙˆÛŒØ±Ú¯ÙˆÙ„ ÙØ§Ø±Ø³ÛŒ
            ",",               # ÙˆÛŒØ±Ú¯ÙˆÙ„ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ
            " Ùˆ ",             # Ø­Ø±Ù Ø±Ø¨Ø· ÙØ§Ø±Ø³ÛŒ
            " Ú©Ù‡ ",            # Ú©Ù„Ù…Ø§Øª Ø±Ø¨Ø· ÙØ§Ø±Ø³ÛŒ
            " Ø¯Ø± ",
            " Ø§Ø² ",
            " Ø¨Ù‡ ",
            " Ø¨Ø§ ",
            " ",               # ÙØ§ØµÙ„Ù‡
            ""                 # Ú©Ø§Ø±Ø§Ú©ØªØ± Ø®Ø§Ù„ÛŒ
        ]
        
        # 3. ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø¨Ù‡ÛŒÙ†Ù‡ Ø¨Ø±Ø§ÛŒ Ù…ØªÙ† ÙØ§Ø±Ø³ÛŒ
        chunk_size = 800       # Ø§Ù†Ø¯Ø§Ø²Ù‡ Ù…Ù†Ø§Ø³Ø¨ Ø¨Ø±Ø§ÛŒ ÙØ§Ø±Ø³ÛŒ
        chunk_overlap = 150    # overlap Ú©Ø§ÙÛŒ Ø¨Ø±Ø§ÛŒ Ø­ÙØ¸ context
        
        # 4. Ø§ÛŒØ¬Ø§Ø¯ text splitter Ø¨Ù‡ÛŒÙ†Ù‡
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            length_function=len,
            separators=persian_separators,
            keep_separator=True,    # Ø­ÙØ¸ Ø¬Ø¯Ø§Ú©Ù†Ù†Ø¯Ù‡â€ŒÙ‡Ø§
            add_start_index=True,   # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø§ÛŒÙ†Ø¯Ú©Ø³ Ø´Ø±ÙˆØ¹
        )
        
        # 5. Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ù…ØªÙ†
        processed_text = preprocess_text(text)
        
        # 6. ØªÙ‚Ø³ÛŒÙ… Ø¨Ù‡ Ú†Ø§Ù†Ú©â€ŒÙ‡Ø§
        chunks = text_splitter.split_text(processed_text)
        
        # 7. Ù¾Ø³â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ú†Ø§Ù†Ú©â€ŒÙ‡Ø§
        def postprocess_chunk(chunk: str) -> str:
            # Ø­Ø°Ù ÙØ§ØµÙ„Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø¨ØªØ¯Ø§ Ùˆ Ø§Ù†ØªÙ‡Ø§
            chunk = chunk.strip()
            # Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² ÙˆØ¬ÙˆØ¯ Ø­Ø¯Ø§Ù‚Ù„ Ø·ÙˆÙ„
            if len(chunk) < 50:
                return None
            # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù†Ù‚Ø·Ù‡ Ø¯Ø± ØµÙˆØ±Øª Ù†ÛŒØ§Ø²
            if not chunk.endswith(('.', '!', 'ØŸ', ':', 'Ø›')):
                chunk += '.'
            return chunk
        
        # 8. ÙÛŒÙ„ØªØ± Ùˆ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ú†Ø§Ù†Ú©â€ŒÙ‡Ø§
        final_chunks = []
        for chunk in chunks:
            processed_chunk = postprocess_chunk(chunk)
            if processed_chunk:
                final_chunks.append(processed_chunk)
        
        # 9. Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¢Ù…Ø§Ø±ÛŒ
        st.write(f"ğŸ“Š Ø¢Ù…Ø§Ø± ØªÙ‚Ø³ÛŒÙ…â€ŒØ¨Ù†Ø¯ÛŒ:")
        st.write(f"   â€¢ ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ Ú†Ø§Ù†Ú©â€ŒÙ‡Ø§: {len(final_chunks)}")
        st.write(f"   â€¢ Ù…ØªÙˆØ³Ø· Ø·ÙˆÙ„ Ú†Ø§Ù†Ú©: {sum(len(chunk) for chunk in final_chunks) // len(final_chunks) if final_chunks else 0} Ú©Ø§Ø±Ø§Ú©ØªØ±")
        st.write(f"   â€¢ Ú©ÙˆØªØ§Ù‡â€ŒØªØ±ÛŒÙ† Ú†Ø§Ù†Ú©: {min(len(chunk) for chunk in final_chunks) if final_chunks else 0} Ú©Ø§Ø±Ø§Ú©ØªØ±")
        st.write(f"   â€¢ Ø¨Ù„Ù†Ø¯ØªØ±ÛŒÙ† Ú†Ø§Ù†Ú©: {max(len(chunk) for chunk in final_chunks) if final_chunks else 0} Ú©Ø§Ø±Ø§Ú©ØªØ±")
        
        return final_chunks

    # 10. ØªØ§Ø¨Ø¹ Ø§Ø¶Ø§ÙÛŒ Ø¨Ø±Ø§ÛŒ Ø¨Ø±Ø±Ø³ÛŒ Ú©ÛŒÙÛŒØª Ú†Ø§Ù†Ú©â€ŒÙ‡Ø§
    def analyze_chunks_quality(self, chunks: List[str]) -> dict:
        """ØªØ­Ù„ÛŒÙ„ Ú©ÛŒÙÛŒØª Ú†Ø§Ù†Ú©â€ŒÙ‡Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯ Ø´Ø¯Ù‡"""
        
        analysis = {
            'total_chunks': len(chunks),
            'avg_length': sum(len(chunk) for chunk in chunks) / len(chunks) if chunks else 0,
            'min_length': min(len(chunk) for chunk in chunks) if chunks else 0,
            'max_length': max(len(chunk) for chunk in chunks) if chunks else 0,
            'empty_chunks': sum(1 for chunk in chunks if len(chunk.strip()) == 0),
            'short_chunks': sum(1 for chunk in chunks if len(chunk) < 100),
            'optimal_chunks': sum(1 for chunk in chunks if 200 <= len(chunk) <= 1000),
        }
        
        return analysis            
                
    def display_app(self):
        st.set_page_config(page_title="Persian NotebookLM ğŸ“š", page_icon= "content/PARS-LM-NOTEBOOK.png")
        st.title("Persian NotebookLM ğŸ“š")
        st.write(self.translations[self.language]['current_language'])
        st.header(self.translations[self.language]['app_title'], divider="red") 
        uploaded_file = st.file_uploader(f"{self.translations[self.language]['select_file']} PDF", type=["pdf"])
        
        if uploaded_file is not None:
            # Save the uploaded file temporarily
            file_path = Path("temp.pdf")
            with open(file_path, "wb") as f:
                f.write(uploaded_file.getbuffer())
            
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            extracted_text = self.extract_text_from_pdf(file_path)
            
            # âœ… Ù…Ø±Ø­Ù„Ù‡ Ø¬Ø¯ÛŒØ¯: ØªÙ‚Ø³ÛŒÙ… Ù…ØªÙ† Ø¨Ù‡ Ú†Ø§Ù†Ú©â€ŒÙ‡Ø§
            progress_bar.progress(50)
            status_text.text("Ø¯Ø± Ø­Ø§Ù„ ØªÙ‚Ø³ÛŒÙ… Ù…ØªÙ† Ø¨Ù‡ Ø¨Ø®Ø´â€ŒÙ‡Ø§ÛŒ Ú©ÙˆÚ†Ú©ØªØ±...")

            #  # Display extracted text
            st.subheader("Ù…ØªÙ† Ø§Ø³ØªØ®Ø±Ø§Ø¬â€ŒØ´Ø¯Ù‡:")
            st.text_area("Ù…ØªÙ†", extracted_text, height=400)

            text_chunks = self.advanced_persian_chunking(extracted_text)

            st.write(text_chunks)
            
            # Clean up temporary file
            file_path.unlink()                     

if __name__ == "__main__":
    app = App()
    app.display_app()