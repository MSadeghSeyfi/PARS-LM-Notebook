import streamlit as st
from langchain_community.document_loaders import PyPDFium2Loader
from pathlib import Path
from langchain.text_splitter import RecursiveCharacterTextSplitter
from typing import List
import re
from RAG import PersianRAGSystem
from LLM import PersianLLMGenerator, ConversationManager

class App:
    def __init__(self):
        self.translations = {
            'fa' : {
                'change_language': 'ØªØºÛŒÛŒØ± Ø²Ø¨Ø§Ù†',
                'current_language': 'Ø²Ø¨Ø§Ù† ÙØ¹Ù„ÛŒ: ÙØ§Ø±Ø³ÛŒ',
                'app_title': 'Ú©Ø§Ø± Ø¨Ø§ PDF',
                'placeholder_genre': 'Ù„Ø·ÙØ§ Ø¯Ø± Ø¨Ø§Ø±Ù‡ ÛŒ PDF Ø³ÙˆØ§Ù„ÛŒ Ø¨Ù¾Ø±Ø³ÛŒØ¯ .',
                'select_file' : 'ÙŠÚ© ÙØ§ÙŠÙ„ PDf Ø¨Ø±Ú¯Ø²ÙŠÙ†ÙŠØ¯ .'
            },
            'en' : {
                'change_language': 'Change Language',
                'current_language': 'Current Language: English',
                'app_title': 'work with PDF',
                'placeholder_genre': 'Please ask a question about pdf .',
                'select_file' : 'Please select a PDF file .'
            }
        }

        if 'language' not in st.session_state:
            st.session_state.language = 'fa'

        if st.button(self.translations[st.session_state.language]['change_language']):
            st.session_state.language = 'en' if st.session_state.language == 'fa' else "fa"

        self.language = st.session_state.language
        self.text_align, self.direction = self.get_text_alignment()
        self.inject_css()
        self.model = None

    def get_text_alignment(self):
            if self.language == 'fa':
                return 'right', 'rtl'
            else:
                return 'left', 'ltr'    
        
    def inject_css(self):
            st.markdown(f"""
                <style>
                        
                    @import url('https://fonts.googleapis.com/css2?family=Vazirmatn:wght@400;500&display=swap');
                    
                    * {{
                        font-family: 'Vazirmatn', sans-serif;
                        direction: {self.direction};
                        text-align: {self.text_align};
                    }}

                </style>
            """, unsafe_allow_html=True)  

    def minimal_persian_fix(self, text):
                # Ascending
                # ØªØµØ­ÛŒØ­ Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§ÛŒ Ø§Ø´ØªØ¨Ø§Ù‡
                char_fixes = {
                    'Í·': 'Ú©',  # Ú©Ø§Ø±Ø§Ú©ØªØ± Ø§Ø´ØªØ¨Ø§Ù‡ Ø¨Ù‡ Ø¬Ø§ÛŒ Ú©
                    'Íº': 'Ú©',  # Ú©Ø§Ø±Ø§Ú©ØªØ± Ø§Ø´ØªØ¨Ø§Ù‡ Ø¯ÛŒÚ¯Ø±
                    'Í½': 'Ú¯',  # Ú©Ø§Ø±Ø§Ú©ØªØ± Ø§Ø´ØªØ¨Ø§Ù‡ Ø¨Ù‡ Ø¬Ø§ÛŒ Ú¯
                    'Ø¤': 'Ùˆ',  # Ùˆ Ù‡Ù…Ø²Ù‡â€ŒØ¯Ø§Ø± Ø¹Ø±Ø¨ÛŒ Ø¨Ù‡ ÙØ§Ø±Ø³ÛŒ
                    'Ù‰': 'ÛŒ',  # ÛŒ Ø¹Ø±Ø¨ÛŒ Ø¨Ù‡ ÙØ§Ø±Ø³ÛŒ
                    'Ø©': 'Ù‡',  # ØªÙ‡ Ù…Ø±Ø¨ÙˆØ·Ù‡ Ø¹Ø±Ø¨ÛŒ Ø¨Ù‡ ÙØ§Ø±Ø³ÛŒ
                    'Ùƒ': 'Ú©',  # Ú©Ø§Ù Ø¹Ø±Ø¨ÛŒ Ø¨Ù‡ ÙØ§Ø±Ø³ÛŒ
                    'ÙŠ': 'ÛŒ',  # ÛŒØ§ Ø¹Ø±Ø¨ÛŒ Ø¨Ù‡ ÙØ§Ø±Ø³ÛŒ
                }

                # Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ†ÛŒ Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§
                for wrong, correct in char_fixes.items():
                    text = text.replace(wrong, correct)

                # ØªØµØ­ÛŒØ­ Ù…Ø´Ú©Ù„ Ø®Ø§Øµ chr(876) + ÙØ§ØµÙ„Ù‡
                text = text.replace(chr(876) + ' ', 'ÛŒ ')
                text = text.replace(chr(876) + chr(13), 'ÛŒ ')
                text = text.replace(chr(876), 'ÛŒ')  # Ø¨Ø±Ø§ÛŒ Ù…ÙˆØ§Ø±Ø¯ Ø¯ÛŒÚ¯Ø± chr(876)

                return text

    def extract_text_from_pdf(self, pdf_path: str) -> str:
            try:
                # Load the PDF using PyPDFium2Loader
                loader = PyPDFium2Loader(pdf_path)
                documents = loader.load()
                text = ""

                # Iterate through all documents (each represents a page)
                for doc in documents:
                    original_content = doc.page_content
                    corrected_content = self.minimal_persian_fix(original_content)
                    text += corrected_content + "\n"  # Add newline between pages
                
                return text
            except Exception as e:
                return f"Error extracting text: {str(e)}"
        

    def advanced_persian_chunking(self, text: str) -> List[str]:
        """
        Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡ ØªÙ‚Ø³ÛŒÙ… Ù…ØªÙ† ÙØ§Ø±Ø³ÛŒ Ø¨Ù‡ Ú†Ø§Ù†Ú©â€ŒÙ‡Ø§ÛŒ Ø¨Ù‡ÛŒÙ†Ù‡
        """
        
        # 1. Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ø§ÙˆÙ„ÛŒÙ‡ Ù…ØªÙ†
        def preprocess_text(text: str) -> str:
            # Ø­Ø°Ù ÙØ§ØµÙ„Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø¶Ø§ÙÛŒ
            text = re.sub(r'\s+', ' ', text)
            # ØªÙ†Ø¸ÛŒÙ… ÙØ§ØµÙ„Ù‡â€ŒÙ‡Ø§ Ù‚Ø¨Ù„ Ùˆ Ø¨Ø¹Ø¯ Ø¹Ù„Ø§Ø¦Ù… Ù†Ú¯Ø§Ø±Ø´ÛŒ ÙØ§Ø±Ø³ÛŒ
            text = re.sub(r'\s*([.!?ØŸ:Ø›ØŒ])\s*', r'\1 ', text)
            # Ø­Ø°Ù Ø®Ø·ÙˆØ· Ø®Ø§Ù„ÛŒ Ø§Ø¶Ø§ÙÛŒ
            text = re.sub(r'\n\s*\n', '\n\n', text)
            return text.strip()
        
        # 2. ØªØ¹Ø±ÛŒÙ Ø¬Ø¯Ø§Ú©Ù†Ù†Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ù‡ÛŒÙ†Ù‡ Ø¨Ø±Ø§ÛŒ ÙØ§Ø±Ø³ÛŒ
        persian_separators = [
            "\n\n\n",          # Ù¾Ø§Ø±Ø§Ú¯Ø±Ø§Ùâ€ŒÙ‡Ø§ÛŒ Ø¬Ø¯Ø§
            "\n\n",            # Ù¾Ø§Ø±Ø§Ú¯Ø±Ø§Ùâ€ŒÙ‡Ø§
            "\n",              # Ø®Ø·ÙˆØ· Ø¬Ø¯ÛŒØ¯
            "ØŸ",               # Ø¹Ù„Ø§Ù…Øª Ø³ÙˆØ§Ù„ ÙØ§Ø±Ø³ÛŒ
            "!",               # Ø¹Ù„Ø§Ù…Øª ØªØ¹Ø¬Ø¨
            ".",               # Ù†Ù‚Ø·Ù‡
            "Ø›",               # Ù†Ù‚Ø·Ù‡ ÙˆÛŒØ±Ú¯ÙˆÙ„ ÙØ§Ø±Ø³ÛŒ
            ":",               # Ø¯Ùˆ Ù†Ù‚Ø·Ù‡
            "ØŒ",               # ÙˆÛŒØ±Ú¯ÙˆÙ„ ÙØ§Ø±Ø³ÛŒ
            ",",               # ÙˆÛŒØ±Ú¯ÙˆÙ„ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ
            " Ùˆ ",             # Ø­Ø±Ù Ø±Ø¨Ø· ÙØ§Ø±Ø³ÛŒ
            " Ú©Ù‡ ",            # Ú©Ù„Ù…Ø§Øª Ø±Ø¨Ø· ÙØ§Ø±Ø³ÛŒ
            " Ø¯Ø± ",
            " Ø§Ø² ",
            " Ø¨Ù‡ ",
            " Ø¨Ø§ ",
            " ",               # ÙØ§ØµÙ„Ù‡
            ""                 # Ú©Ø§Ø±Ø§Ú©ØªØ± Ø®Ø§Ù„ÛŒ
        ]
        
        # 3. ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø¨Ù‡ÛŒÙ†Ù‡ Ø¨Ø±Ø§ÛŒ Ù…ØªÙ† ÙØ§Ø±Ø³ÛŒ
        chunk_size = 800       # Ø§Ù†Ø¯Ø§Ø²Ù‡ Ù…Ù†Ø§Ø³Ø¨ Ø¨Ø±Ø§ÛŒ ÙØ§Ø±Ø³ÛŒ
        chunk_overlap = 150    # overlap Ú©Ø§ÙÛŒ Ø¨Ø±Ø§ÛŒ Ø­ÙØ¸ context
        
        # 4. Ø§ÛŒØ¬Ø§Ø¯ text splitter Ø¨Ù‡ÛŒÙ†Ù‡
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            length_function=len,
            separators=persian_separators,
            keep_separator=True,    # Ø­ÙØ¸ Ø¬Ø¯Ø§Ú©Ù†Ù†Ø¯Ù‡â€ŒÙ‡Ø§
            add_start_index=True,   # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø§ÛŒÙ†Ø¯Ú©Ø³ Ø´Ø±ÙˆØ¹
        )
        
        # 5. Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ù…ØªÙ†
        processed_text = preprocess_text(text)
        
        # 6. ØªÙ‚Ø³ÛŒÙ… Ø¨Ù‡ Ú†Ø§Ù†Ú©â€ŒÙ‡Ø§
        chunks = text_splitter.split_text(processed_text)
        
        # 7. Ù¾Ø³â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ú†Ø§Ù†Ú©â€ŒÙ‡Ø§
        def postprocess_chunk(chunk: str) -> str:
            # Ø­Ø°Ù ÙØ§ØµÙ„Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø¨ØªØ¯Ø§ Ùˆ Ø§Ù†ØªÙ‡Ø§
            chunk = chunk.strip()
            # Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² ÙˆØ¬ÙˆØ¯ Ø­Ø¯Ø§Ù‚Ù„ Ø·ÙˆÙ„
            if len(chunk) < 50:
                return None
            # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù†Ù‚Ø·Ù‡ Ø¯Ø± ØµÙˆØ±Øª Ù†ÛŒØ§Ø²
            if not chunk.endswith(('.', '!', 'ØŸ', ':', 'Ø›')):
                chunk += '.'
            return chunk
        
        # 8. ÙÛŒÙ„ØªØ± Ùˆ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ú†Ø§Ù†Ú©â€ŒÙ‡Ø§
        final_chunks = []
        for chunk in chunks:
            processed_chunk = postprocess_chunk(chunk)
            if processed_chunk:
                final_chunks.append(processed_chunk)
        
        # 9. Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¢Ù…Ø§Ø±ÛŒ
        st.write(f"ğŸ“Š Ø¢Ù…Ø§Ø± ØªÙ‚Ø³ÛŒÙ…â€ŒØ¨Ù†Ø¯ÛŒ:")
        st.write(f"   â€¢ ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ Ú†Ø§Ù†Ú©â€ŒÙ‡Ø§: {len(final_chunks)}")
        st.write(f"   â€¢ Ù…ØªÙˆØ³Ø· Ø·ÙˆÙ„ Ú†Ø§Ù†Ú©: {sum(len(chunk) for chunk in final_chunks) // len(final_chunks) if final_chunks else 0} Ú©Ø§Ø±Ø§Ú©ØªØ±")
        st.write(f"   â€¢ Ú©ÙˆØªØ§Ù‡â€ŒØªØ±ÛŒÙ† Ú†Ø§Ù†Ú©: {min(len(chunk) for chunk in final_chunks) if final_chunks else 0} Ú©Ø§Ø±Ø§Ú©ØªØ±")
        st.write(f"   â€¢ Ø¨Ù„Ù†Ø¯ØªØ±ÛŒÙ† Ú†Ø§Ù†Ú©: {max(len(chunk) for chunk in final_chunks) if final_chunks else 0} Ú©Ø§Ø±Ø§Ú©ØªØ±")
        
        return final_chunks

    # 10. ØªØ§Ø¨Ø¹ Ø§Ø¶Ø§ÙÛŒ Ø¨Ø±Ø§ÛŒ Ø¨Ø±Ø±Ø³ÛŒ Ú©ÛŒÙÛŒØª Ú†Ø§Ù†Ú©â€ŒÙ‡Ø§
    def analyze_chunks_quality(self, chunks: List[str]) -> dict:
        """ØªØ­Ù„ÛŒÙ„ Ú©ÛŒÙÛŒØª Ú†Ø§Ù†Ú©â€ŒÙ‡Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯ Ø´Ø¯Ù‡"""
        
        analysis = {
            'total_chunks': len(chunks),
            'avg_length': sum(len(chunk) for chunk in chunks) / len(chunks) if chunks else 0,
            'min_length': min(len(chunk) for chunk in chunks) if chunks else 0,
            'max_length': max(len(chunk) for chunk in chunks) if chunks else 0,
            'empty_chunks': sum(1 for chunk in chunks if len(chunk.strip()) == 0),
            'short_chunks': sum(1 for chunk in chunks if len(chunk) < 100),
            'optimal_chunks': sum(1 for chunk in chunks if 200 <= len(chunk) <= 1000),
        }
        
        return analysis
    
    def Grok_API_key(self):
        api_key = st.sidebar.text_input("Grok API Key", type="password")
        if not api_key:
            st.info("Please enter Grok API key to continue.")
            st.stop()
        return api_key

    def display_app(self):
        st.set_page_config(page_title="Persian NotebookLM ğŸ“š", page_icon= "content/PARS-LM-NOTEBOOK.png")
        st.title("Persian NotebookLM ğŸ“š")
        st.write(self.translations[self.language]['current_language'])
        st.header(self.translations[self.language]['app_title'], divider="red") 
        Grok_api = self.Grok_API_key()  # Ø¯Ø±ÛŒØ§ÙØª API key Ø§Ø² Ú©Ø§Ø±Ø¨Ø±
        if Grok_api:
            st.session_state["grok_api_key"] = Grok_api  # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± session state
            
        uploaded_file = st.file_uploader(f"{self.translations[self.language]['select_file']} PDF", type=["pdf"])
        
        if uploaded_file is not None:
            # Save the uploaded file temporarily
            file_path = Path("temp.pdf")
            with open(file_path, "wb") as f:
                f.write(uploaded_file.getbuffer())
            
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            extracted_text = self.extract_text_from_pdf(file_path)
            
            # âœ… Ù…Ø±Ø­Ù„Ù‡ Ø¬Ø¯ÛŒØ¯: ØªÙ‚Ø³ÛŒÙ… Ù…ØªÙ† Ø¨Ù‡ Ú†Ø§Ù†Ú©â€ŒÙ‡Ø§
            progress_bar.progress(50)
            status_text.text("Ø¯Ø± Ø­Ø§Ù„ ØªÙ‚Ø³ÛŒÙ… Ù…ØªÙ† Ø¨Ù‡ Ø¨Ø®Ø´â€ŒÙ‡Ø§ÛŒ Ú©ÙˆÚ†Ú©ØªØ±...")

            #  # Display extracted text
            st.subheader("Ù…ØªÙ† Ø§Ø³ØªØ®Ø±Ø§Ø¬â€ŒØ´Ø¯Ù‡:")
            st.text_area("Ù…ØªÙ†", extracted_text, height=400)

            text_chunks = self.advanced_persian_chunking(extracted_text)

            progress_bar.progress(75)
            status_text.text("Ø¯Ø± Ø­Ø§Ù„ Ø§ÛŒØ¬Ø§Ø¯ Ø¨Ø±Ø¯Ø§Ø±Ù‡Ø§ÛŒ embedding...")

            rag_system = get_rag_system()
            rag_system.add_documents(text_chunks)
            
            progress_bar.progress(100)
            status_text.text("Ø³ÛŒØ³ØªÙ… RAG Ø¢Ù…Ø§Ø¯Ù‡ Ø§Ø³Øª!")
            
            # âœ… Ù…Ø±Ø­Ù„Ù‡ Ø¬Ø¯ÛŒØ¯: LLM Integration
            st.subheader("ğŸ’¬ Ù¾Ø±Ø³Ø´ Ùˆ Ù¾Ø§Ø³Ø® Ù‡ÙˆØ´Ù…Ù†Ø¯")
            
            # Ø§ÛŒØ¬Ø§Ø¯ session state Ø¨Ø±Ø§ÛŒ ØªØ§Ø±ÛŒØ®Ú†Ù‡
            if 'conversation_manager' not in st.session_state:
                st.session_state.conversation_manager = ConversationManager()
            
            # Ø¯Ø±ÛŒØ§ÙØª Ø³ÙˆØ§Ù„ Ø§Ø² Ú©Ø§Ø±Ø¨Ø±
            user_question = st.text_input("ğŸ¤” Ø³ÙˆØ§Ù„ Ø®ÙˆØ¯ Ø±Ø§ Ø¯Ø± Ù…ÙˆØ±Ø¯ Ù…Ø­ØªÙˆØ§ÛŒ PDF Ø¨Ù¾Ø±Ø³ÛŒØ¯:")
            
            col1, col2 = st.columns([1, 4])
            with col1:
                ask_button = st.button("â“ Ù¾Ø±Ø³ÛŒØ¯Ù†", type="primary")
            with col2:
                clear_button = st.button("ğŸ—‘ï¸ Ù¾Ø§Ú© Ú©Ø±Ø¯Ù† ØªØ§Ø±ÛŒØ®Ú†Ù‡")
            
            if clear_button:
                st.session_state.conversation_manager.clear_history()
                st.success("âœ… ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ù…Ú©Ø§Ù„Ù…Ø§Øª Ù¾Ø§Ú© Ø´Ø¯")
            
            if ask_button and user_question:
                with st.spinner("ğŸ¤– Ø¯Ø± Ø­Ø§Ù„ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø³ÙˆØ§Ù„..."):
                    # Ø¯Ø±ÛŒØ§ÙØª LLM generator
                    llm_generator = get_llm_generator()
                    
                    # Ø¯Ø±ÛŒØ§ÙØª ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ø§Ø®ÛŒØ±
                    recent_history = st.session_state.conversation_manager.get_recent_history()
                    
                    # Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø³ÙˆØ§Ù„ Ø¨Ø§ RAG
                    response = llm_generator.query_with_rag(
                        rag_system=rag_system,
                        query=user_question,
                        top_k=5,
                        conversation_history=recent_history
                    )
                    
                    # Ù†Ù…Ø§ÛŒØ´ Ù¾Ø§Ø³Ø®
                    st.subheader("ğŸ¯ Ù¾Ø§Ø³Ø®:")
                    st.write(response['answer'])
                    
                    # Ù†Ù…Ø§ÛŒØ´ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø§Ø¶Ø§ÙÛŒ
                    if 'model_info' in response:
                        with st.expander("ğŸ“Š Ø¬Ø²Ø¦ÛŒØ§Øª ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø®"):
                            col1, col2, col3 = st.columns(3)
                            with col1:
                                st.metric("Ù…Ù†Ø§Ø¨Ø¹ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø´Ø¯Ù‡", response['sources_used'])
                            with col2:
                                st.metric("Ø²Ù…Ø§Ù† ØªÙˆÙ„ÛŒØ¯", f"{response['generation_time']}s")
                            with col3:
                                st.metric("ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ÛŒ Ú©Ù„", response['model_info']['tokens_used']['total_tokens'])
                    
                    # Ù†Ù…Ø§ÛŒØ´ Ù…Ù†Ø§Ø¨Ø¹ Ù…Ø±Ø¬Ø¹
                    if 'retrieval_context' in response and response['retrieval_context']:
                        with st.expander("ğŸ“š Ù…Ù†Ø§Ø¨Ø¹ Ù…Ø±Ø¬Ø¹"):
                            for source in response['retrieval_context']:
                                st.write(f"**Ù…Ù†Ø¨Ø¹ {source['source_id']}** (Ø§Ù…ØªÛŒØ§Ø²: {source['similarity']:.3f})")
                                st.write(source['preview'])
                                st.divider()
                    
                    # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± ØªØ§Ø±ÛŒØ®Ú†Ù‡
                    st.session_state.conversation_manager.add_exchange(
                        user_question, 
                        response['answer']
                    )
            
            # Ù†Ù…Ø§ÛŒØ´ ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ù…Ú©Ø§Ù„Ù…Ø§Øª
            if st.session_state.conversation_manager.conversation_history:
                with st.expander("ğŸ“œ ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ù…Ú©Ø§Ù„Ù…Ø§Øª"):
                    for i, exchange in enumerate(reversed(st.session_state.conversation_manager.conversation_history)):
                        st.write(f"**Ø³ÙˆØ§Ù„ {len(st.session_state.conversation_manager.conversation_history)-i}:** {exchange['user']}")
                        st.write(f"**Ù¾Ø§Ø³Ø®:** {exchange['assistant']}")
                        st.divider()
            
            # Clean up temporary file
            file_path.unlink()                           

@st.cache_resource
def get_rag_system():
    JINA_API_KEY = "jina_f79a39580af2409c9192df3695351ff6-Up2UwFsKAGW1Az1UoKfK2-bJGzA"  # âœ… Ø¯Ø±ÛŒØ§ÙØª API key
    return PersianRAGSystem(JINA_API_KEY)      # âœ… Ù¾Ø§Ø³ Ø¯Ø§Ø¯Ù† API key

@st.cache_resource  
def get_llm_generator():
    GROQ_API_KEY = st.session_state['grok_api_key']  # Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ† Ø¨Ø§ API key ÙˆØ§Ù‚Ø¹ÛŒ
    return PersianLLMGenerator(GROQ_API_KEY)

if __name__ == "__main__":
    app = App()
    app.display_app()