from groq import Groq
from typing import List, Dict, Any, Optional
import time

class PersianLLMGenerator:
    """Ù…ÙˆÙ„Ø¯ Ù¾Ø§Ø³Ø® Ù‡ÙˆØ´Ù…Ù†Ø¯ ÙØ§Ø±Ø³ÛŒ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…Ø¯Ù„ qwen3-32b"""
    
    def __init__(self, groq_api_key: str):
        """Ù…Ù‚Ø¯Ø§Ø±Ø¯Ù‡ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ Ù…ÙˆÙ„Ø¯ LLM"""
        
        if not groq_api_key:
            raise ValueError("âŒ API Key Groq Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø² Ø§Ø³Øª")
            
        self.groq_client = Groq(api_key=groq_api_key)
        self.model_name = "qwen/qwen3-32b"
        
        # ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ø¨Ø±Ø§ÛŒ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ ÙØ§Ø±Ø³ÛŒ
        self.generation_config = {
            "temperature": 0.2,        # Ú©Ø§Ù‡Ø´ ØªØµØ§Ø¯ÙÛŒ Ø¨Ø±Ø§ÛŒ Ø¯Ù‚Øª Ø¨ÛŒØ´ØªØ±
            "max_tokens": 2048,        # Ø·ÙˆÙ„ Ù…Ù†Ø§Ø³Ø¨ Ø¨Ø±Ø§ÛŒ Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ÛŒ ØªÙØµÛŒÙ„ÛŒ ÙØ§Ø±Ø³ÛŒ
            "top_p": 0.9,              # Ú©Ù†ØªØ±Ù„ ØªÙ†ÙˆØ¹ Ù¾Ø§Ø³Ø®
            "frequency_penalty": 0.1,   # Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² ØªÚ©Ø±Ø§Ø±
            "presence_penalty": 0.1     # ØªØ´ÙˆÛŒÙ‚ Ø¨Ù‡ ØªÙ†ÙˆØ¹ Ù…Ø­ØªÙˆØ§
        }
        
        # print("âœ… Ù…ÙˆÙ„Ø¯ LLM ÙØ§Ø±Ø³ÛŒ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ù…Ù‚Ø¯Ø§Ø±Ø¯Ù‡ÛŒ Ø´Ø¯")
        # print(f"ğŸ¤– Ù…Ø¯Ù„: {self.model_name}")

    def _build_persian_prompt(self, query: str, retrieved_chunks: List[Dict[str, Any]], 
                            conversation_history: Optional[List[Dict]] = None) -> str:
        """Ø³Ø§Ø®Øª prompt Ø¨Ù‡ÛŒÙ†Ù‡ Ø´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ø²Ø¨Ø§Ù† ÙØ§Ø±Ø³ÛŒ"""
        
        # Ø³Ø§Ø®Øª context Ø§Ø² Ú†Ø§Ù†Ú©â€ŒÙ‡Ø§ÛŒ Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ø´Ø¯Ù‡
        context_parts = []
        for i, chunk in enumerate(retrieved_chunks, 1):
            similarity_score = chunk.get('similarity_score', 0)
            text = chunk.get('text', '')
            context_parts.append(f"Ù…Ù†Ø¨Ø¹ {i} (Ø§Ù…ØªÛŒØ§Ø²: {similarity_score:.3f}):\n{text}")
        
        context = "\n\n".join(context_parts)
        
        # Ù¾Ø§ÛŒÙ‡ prompt Ø¨Ø±Ø§ÛŒ ÙØ§Ø±Ø³ÛŒ
        base_prompt = f"""ØªÙˆ ÛŒÚ© Ø¯Ø³ØªÛŒØ§Ø± Ù‡ÙˆØ´Ù…Ù†Ø¯ ÙØ§Ø±Ø³ÛŒ Ù‡Ø³ØªÛŒ Ú©Ù‡ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ø³Ù†Ø§Ø¯ Ø§Ø±Ø§Ø¦Ù‡ Ø´Ø¯Ù‡ Ù¾Ø§Ø³Ø® Ø¯Ù‚ÛŒÙ‚ Ùˆ Ø¬Ø§Ù…Ø¹ Ù…ÛŒâ€ŒØ¯Ù‡ÛŒ.

ğŸ“‹ **Ø§Ø³Ù†Ø§Ø¯ Ù…Ø±Ø¬Ø¹:**
{context}

ğŸ¯ **Ø±Ù‡Ù†Ù…ÙˆØ¯Ù‡Ø§ÛŒ Ù…Ù‡Ù…:**
1. Ù¾Ø§Ø³Ø®Øª Ø±Ø§ ÙÙ‚Ø· Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ø³Ù†Ø§Ø¯ Ø§Ø±Ø§Ø¦Ù‡ Ø´Ø¯Ù‡ Ø¨Ø¯Ù‡
2. Ø§Ú¯Ø± Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ú©Ø§ÙÛŒ Ø¯Ø± Ø§Ø³Ù†Ø§Ø¯ Ù†ÛŒØ³ØªØŒ ØµØ§Ø¯Ù‚Ø§Ù†Ù‡ Ø¨Ú¯Ùˆ
3. Ù¾Ø§Ø³Ø® Ø±Ø§ Ø¨Ù‡ Ø²Ø¨Ø§Ù† ÙØ§Ø±Ø³ÛŒ Ùˆ Ø¨Ø§ Ø³Ø§Ø®ØªØ§Ø± Ù…Ù†Ø·Ù‚ÛŒ Ø§Ø±Ø§Ø¦Ù‡ Ú©Ù†
4. Ø§Ø² Ù…Ù†Ø§Ø¨Ø¹ Ù…ÙˆØ¬ÙˆØ¯ Ù†Ù‚Ù„ Ù‚ÙˆÙ„ Ù…Ù†Ø§Ø³Ø¨ Ø§Ø±Ø§Ø¦Ù‡ Ø¯Ù‡
5. Ø§Ú¯Ø± Ø³ÙˆØ§Ù„ Ú†Ù†Ø¯Ø¨Ø®Ø´ÛŒ Ø§Ø³ØªØŒ Ù¾Ø§Ø³Ø® Ø±Ø§ Ø¨Ø®Ø´â€ŒØ¨Ù†Ø¯ÛŒ Ú©Ù†

â“ **Ø³ÙˆØ§Ù„ Ú©Ø§Ø±Ø¨Ø±:** {query}

ğŸ’¡ **Ù¾Ø§Ø³Ø® Ø¬Ø§Ù…Ø¹:**"""

        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ù…Ú©Ø§Ù„Ù…Ù‡ Ø¯Ø± ØµÙˆØ±Øª ÙˆØ¬ÙˆØ¯
        if conversation_history:
            history_text = "\n".join([
                f"Ú©Ø§Ø±Ø¨Ø±: {msg['user']}\nØ¯Ø³ØªÛŒØ§Ø±: {msg['assistant']}" 
                for msg in conversation_history[-3:]  # Ø¢Ø®Ø±ÛŒÙ† 3 Ù…Ú©Ø§Ù„Ù…Ù‡
            ])
            base_prompt = f"""ğŸ“œ **ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ù…Ú©Ø§Ù„Ù…Ù‡ Ø§Ø®ÛŒØ±:**
{history_text}

{base_prompt}"""

        return base_prompt

    def generate_response(self, query: str, retrieved_chunks: List[Dict[str, Any]], 
                         conversation_history: Optional[List[Dict]] = None) -> Dict[str, Any]:
        """ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø¨Ø± Ø§Ø³Ø§Ø³ context Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ø´Ø¯Ù‡"""
        
        # print(f"ğŸ¤– Ø¯Ø± Ø­Ø§Ù„ ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® Ø¨Ø±Ø§ÛŒ: {query[:50]}...")
        
        # Ø³Ø§Ø®Øª prompt
        prompt = self._build_persian_prompt(query, retrieved_chunks, conversation_history)
        
        try:
            # Ø§Ø±Ø³Ø§Ù„ Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø¨Ù‡ Groq
            start_time = time.time()
            
            response = self.groq_client.chat.completions.create(
                model=self.model_name,
                messages=[
                    {
                        "role": "system",
                        "content": "ØªÙˆ ÛŒÚ© Ø¯Ø³ØªÛŒØ§Ø± Ù‡ÙˆØ´Ù…Ù†Ø¯ ÙØ§Ø±Ø³ÛŒ Ù‡Ø³ØªÛŒ Ú©Ù‡ Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ÛŒ Ø¯Ù‚ÛŒÙ‚ Ùˆ Ù…ÙØµÙ„ Ø§Ø±Ø§Ø¦Ù‡ Ù…ÛŒâ€ŒØ¯Ù‡ÛŒ. Ù‡Ù…ÛŒØ´Ù‡ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù…Ù†Ø§Ø¨Ø¹ Ø§Ø±Ø§Ø¦Ù‡ Ø´Ø¯Ù‡ Ù¾Ø§Ø³Ø® Ø¨Ø¯Ù‡ Ùˆ Ø§Ú¯Ø± Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ú©Ø§ÙÛŒ Ù†Ø¯Ø§Ø±ÛŒØŒ ØµØ§Ø¯Ù‚Ø§Ù†Ù‡ Ø§Ø¹Ù„Ø§Ù… Ú©Ù†."
                    },
                    {
                        "role": "user", 
                        "content": prompt
                    }
                ],
                **self.generation_config
            )
            
            end_time = time.time()
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù¾Ø§Ø³Ø®
            generated_answer = response.choices[0].message.content
            
            # Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù†ØªÛŒØ¬Ù‡ Ù†Ù‡Ø§ÛŒÛŒ
            result = {
                'answer': generated_answer,
                'sources_used': len(retrieved_chunks),
                'generation_time': round(end_time - start_time, 2),
                'model_info': {
                    'model': self.model_name,
                    'tokens_used': {
                        'prompt_tokens': response.usage.prompt_tokens,
                        'completion_tokens': response.usage.completion_tokens,
                        'total_tokens': response.usage.total_tokens
                    }
                },
                'retrieval_context': [
                    {
                        'source_id': i,
                        'similarity': chunk.get('similarity_score', 0),
                        'preview': chunk.get('text', '')[:200] + "..."
                    }
                    for i, chunk in enumerate(retrieved_chunks, 1)
                ]
            }
            
            # print(f"âœ… Ù¾Ø§Ø³Ø® Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª ØªÙˆÙ„ÛŒØ¯ Ø´Ø¯")
            # print(f"â±ï¸ Ø²Ù…Ø§Ù† ØªÙˆÙ„ÛŒØ¯: {result['generation_time']} Ø«Ø§Ù†ÛŒÙ‡")
            # print(f"ğŸ“Š ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø´Ø¯Ù‡: {result['model_info']['tokens_used']['total_tokens']}")
            
            return result
            
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø®: {str(e)}")
            return {
                'answer': f"âš ï¸ Ù…ØªØ£Ø³ÙØ§Ù†Ù‡ Ø¯Ø± ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® Ø®Ø·Ø§ÛŒÛŒ Ø±Ø® Ø¯Ø§Ø¯: {str(e)}",
                'sources_used': 0,
                'generation_time': 0,
                'error': str(e)
            }

    def query_with_rag(self, rag_system, query: str, top_k: int = 5, 
                      conversation_history: Optional[List[Dict]] = None) -> Dict[str, Any]:
        """Ù¾Ø±Ø¯Ø§Ø²Ø´ Ú©Ø§Ù…Ù„ Ø³ÙˆØ§Ù„ Ø¨Ø§ Ø³ÛŒØ³ØªÙ… RAG"""
        
        # print(f"ğŸ”„ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø³ÙˆØ§Ù„ RAG: {query}")
        
        try:
            # Ù…Ø±Ø­Ù„Ù‡ 1: Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ø§Ø³Ù†Ø§Ø¯ Ù…Ø±ØªØ¨Ø·
            retrieved_chunks = rag_system.search_similar_chunks(query, top_k=top_k)
            
            if not retrieved_chunks:
                return {
                    'answer': "âš ï¸ Ù…ØªØ£Ø³ÙØ§Ù†Ù‡ Ø§Ø³Ù†Ø§Ø¯ Ù…Ø±ØªØ¨Ø·ÛŒ Ø¨Ø±Ø§ÛŒ Ù¾Ø§Ø³Ø® Ø¨Ù‡ Ø³ÙˆØ§Ù„ Ø´Ù…Ø§ ÛŒØ§ÙØª Ù†Ø´Ø¯.",
                    'sources_used': 0,
                    'retrieval_context': []
                }
            
            # Ù…Ø±Ø­Ù„Ù‡ 2: ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø®
            response = self.generate_response(query, retrieved_chunks, conversation_history)
            
            return response
            
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ RAG: {str(e)}")
            return {
                'answer': f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø³ÙˆØ§Ù„: {str(e)}",
                'error': str(e)
            }

# Ú©Ù„Ø§Ø³ Ù…Ø¯ÛŒØ±ÛŒØª Ù…Ú©Ø§Ù„Ù…Ù‡
class ConversationManager:
    """Ù…Ø¯ÛŒØ±ÛŒØª ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ù…Ú©Ø§Ù„Ù…Ø§Øª"""
    
    def __init__(self, max_history: int = 10):
        self.conversation_history = []
        self.max_history = max_history
    
    def add_exchange(self, user_query: str, assistant_response: str):
        """Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù…Ú©Ø§Ù„Ù…Ù‡ Ø¬Ø¯ÛŒØ¯"""
        self.conversation_history.append({
            'user': user_query,
            'assistant': assistant_response,
            'timestamp': time.time()
        })
        
        # Ø­ÙØ¸ Ø­Ø¯Ø§Ú©Ø«Ø± ØªØ¹Ø¯Ø§Ø¯ Ù…Ú©Ø§Ù„Ù…Ù‡
        if len(self.conversation_history) > self.max_history:
            self.conversation_history = self.conversation_history[-self.max_history:]
    
    def get_recent_history(self, count: int = 3) -> List[Dict]:
        """Ø¯Ø±ÛŒØ§ÙØª Ø¢Ø®Ø±ÛŒÙ† Ù…Ú©Ø§Ù„Ù…Ø§Øª"""
        return self.conversation_history[-count:] if self.conversation_history else []
    
    def clear_history(self):
        """Ù¾Ø§Ú© Ú©Ø±Ø¯Ù† ØªØ§Ø±ÛŒØ®Ú†Ù‡"""
        self.conversation_history.clear()