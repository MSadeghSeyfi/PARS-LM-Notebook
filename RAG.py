import numpy as np
from sentence_transformers import SentenceTransformer
import faiss
import pickle
from typing import List, Dict, Any
import json

class PersianRAGSystem:
    """Ø³ÛŒØ³ØªÙ… RAG Ø¨Ù‡ÛŒÙ†Ù‡ Ø´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ø²Ø¨Ø§Ù† ÙØ§Ø±Ø³ÛŒ"""
    
    def __init__(self, jina_api_key: str = None):
        """Ù…Ù‚Ø¯Ø§Ø±Ø¯Ù‡ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ Ø³ÛŒØ³ØªÙ… RAG"""
        
        # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ embedding
        self.embedding_model = SentenceTransformer('jinaai/jina-embeddings-v3', 
                                                   trust_remote_code=True)
        
        # ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù…Ø¯Ù„ Ø¨Ø±Ø§ÛŒ Ú†Ù†Ø¯Ø²Ø¨Ø§Ù†Ù‡ Ø¨ÙˆØ¯Ù†
        self.embedding_model.max_seq_length = 8192  # Ø·ÙˆÙ„ Ø¨ÛŒØ´ØªØ± Ø¨Ø±Ø§ÛŒ Ù…ØªÙˆÙ† ÙØ§Ø±Ø³ÛŒ
        
        # Ù…ØªØºÛŒØ±Ù‡Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡â€ŒØ³Ø§Ø²ÛŒ
        self.chunks = []           # Ù„ÛŒØ³Øª Ú†Ø§Ù†Ú©â€ŒÙ‡Ø§ÛŒ Ù…ØªÙ†ÛŒ
        self.embeddings = None     # Ù…Ø§ØªØ±ÛŒØ³ embeddings
        self.vector_index = None   # Ø§ÛŒÙ†Ø¯Ú©Ø³ FAISS
        self.chunk_metadata = []   # Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø§Ø¶Ø§ÙÛŒ Ú†Ø§Ù†Ú©â€ŒÙ‡Ø§
        
        print("âœ… Ø³ÛŒØ³ØªÙ… RAG Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ù…Ù‚Ø¯Ø§Ø±Ø¯Ù‡ÛŒ Ø´Ø¯")
        print(f"ğŸ“Š Ù…Ø¯Ù„ embedding: jina-embeddings-v3")
        print(f"ğŸŒ Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ú†Ù†Ø¯Ø²Ø¨Ø§Ù†Ù‡: ÙØ§Ø±Ø³ÛŒØŒ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒØŒ Ø¹Ø±Ø¨ÛŒ")

    def create_embeddings(self, chunks: List[str]) -> np.ndarray:
        """ØªØ¨Ø¯ÛŒÙ„ Ú†Ø§Ù†Ú©â€ŒÙ‡Ø§ Ø¨Ù‡ Ø¨Ø±Ø¯Ø§Ø±Ù‡Ø§ÛŒ embedding"""
        
        print(f"ğŸ”„ Ø¯Ø± Ø­Ø§Ù„ ØªÙˆÙ„ÛŒØ¯ embeddings Ø¨Ø±Ø§ÛŒ {len(chunks)} Ú†Ø§Ù†Ú©...")
        
        # ØªÙˆÙ„ÛŒØ¯ embeddings
        embeddings = self.embedding_model.encode(
            chunks,
            batch_size=8,           # batch Ú©ÙˆÚ†Ú© Ø¨Ø±Ø§ÛŒ Ù…Ø¯ÛŒØ±ÛŒØª Ø­Ø§ÙØ¸Ù‡
            show_progress_bar=True,
            convert_to_numpy=True,
            normalize_embeddings=True  # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø±Ø§ÛŒ Ø¨Ù‡Ø¨ÙˆØ¯ Ø¬Ø³ØªØ¬Ùˆ
        )
        
        print(f"âœ… ØªÙˆÙ„ÛŒØ¯ embeddings Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯")
        print(f"ğŸ“ Ø§Ø¨Ø¹Ø§Ø¯ Ù‡Ø± embedding: {embeddings.shape[1]}")
        
        return embeddings

    def build_vector_index(self, embeddings: np.ndarray):
        """Ø³Ø§Ø®Øª Ø§ÛŒÙ†Ø¯Ú©Ø³ FAISS Ø¨Ø±Ø§ÛŒ Ø¬Ø³ØªØ¬ÙˆÛŒ Ø³Ø±ÛŒØ¹"""
        
        print("ğŸ—ï¸ Ø¯Ø± Ø­Ø§Ù„ Ø³Ø§Ø®Øª Ø§ÛŒÙ†Ø¯Ú©Ø³ Ø¨Ø±Ø¯Ø§Ø±ÛŒ...")
        
        # ØªØ¹ÛŒÛŒÙ† Ù†ÙˆØ¹ Ø§ÛŒÙ†Ø¯Ú©Ø³ Ø¨Ø± Ø§Ø³Ø§Ø³ ØªØ¹Ø¯Ø§Ø¯ Ú†Ø§Ù†Ú©â€ŒÙ‡Ø§
        dimension = embeddings.shape[1]
        
        if len(embeddings) < 1000:
            # Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ú©Ù…: Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² IndexFlatIP (Ø¯Ù‚ÛŒÙ‚â€ŒØªØ±ÛŒÙ†)
            self.vector_index = faiss.IndexFlatIP(dimension)
        else:
            # Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø²ÛŒØ§Ø¯: Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² IndexIVFFlat (Ø³Ø±ÛŒØ¹â€ŒØªØ±)
            nlist = min(100, len(embeddings) // 10)
            quantizer = faiss.IndexFlatIP(dimension)
            self.vector_index = faiss.IndexIVFFlat(quantizer, dimension, nlist)
            self.vector_index.train(embeddings.astype('float32'))
        
        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† embeddings Ø¨Ù‡ Ø§ÛŒÙ†Ø¯Ú©Ø³
        self.vector_index.add(embeddings.astype('float32'))
        
        print(f"âœ… Ø§ÛŒÙ†Ø¯Ú©Ø³ Ø¨Ø±Ø¯Ø§Ø±ÛŒ Ø³Ø§Ø®ØªÙ‡ Ø´Ø¯: {self.vector_index.ntotal} Ø¨Ø±Ø¯Ø§Ø±")

    def add_documents(self, chunks: List[str]):
        """Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø§Ø³Ù†Ø§Ø¯ Ø¨Ù‡ Ø³ÛŒØ³ØªÙ… RAG"""
        
        # Ø°Ø®ÛŒØ±Ù‡ Ú†Ø§Ù†Ú©â€ŒÙ‡Ø§
        self.chunks = chunks
        
        # Ø§ÛŒØ¬Ø§Ø¯ metadata Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ú†Ø§Ù†Ú©
        self.chunk_metadata = [
            {
                'chunk_id': i,
                'length': len(chunk),
                'preview': chunk[:100] + "..." if len(chunk) > 100 else chunk
            }
            for i, chunk in enumerate(chunks)
        ]
        
        # ØªÙˆÙ„ÛŒØ¯ embeddings
        self.embeddings = self.create_embeddings(chunks)
        
        # Ø³Ø§Ø®Øª Ø§ÛŒÙ†Ø¯Ú©Ø³ Ø¨Ø±Ø¯Ø§Ø±ÛŒ
        self.build_vector_index(self.embeddings)
        
        print(f"ğŸ¯ {len(chunks)} Ø³Ù†Ø¯ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ù‡ Ø³ÛŒØ³ØªÙ… Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯")

    def search_similar_chunks(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:
        """Ø¬Ø³ØªØ¬ÙˆÛŒ Ú†Ø§Ù†Ú©â€ŒÙ‡Ø§ÛŒ Ù…Ø´Ø§Ø¨Ù‡ Ø¨Ø±Ø§ÛŒ Ø³ÙˆØ§Ù„"""
        
        if self.vector_index is None:
            raise ValueError("âŒ Ø§Ø¨ØªØ¯Ø§ Ø¨Ø§ÛŒØ¯ Ø§Ø³Ù†Ø§Ø¯ Ø±Ø§ Ø¨Ù‡ Ø³ÛŒØ³ØªÙ… Ø§Ø¶Ø§ÙÙ‡ Ú©Ù†ÛŒØ¯")
        
        print(f"ğŸ” Ø¬Ø³ØªØ¬Ùˆ Ø¨Ø±Ø§ÛŒ: {query[:50]}...")
        
        # ØªØ¨Ø¯ÛŒÙ„ Ø³ÙˆØ§Ù„ Ø¨Ù‡ embedding
        query_embedding = self.embedding_model.encode(
            [query],
            convert_to_numpy=True,
            normalize_embeddings=True
        )
        
        # Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± Ø§ÛŒÙ†Ø¯Ú©Ø³
        scores, indices = self.vector_index.search(
            query_embedding.astype('float32'), 
            top_k
        )
        
        # Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù†ØªØ§ÛŒØ¬
        results = []
        for i, (score, idx) in enumerate(zip(scores[0], indices[0])):
            if idx >= 0:  # Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² ÙˆØ¬ÙˆØ¯ Ø§ÛŒÙ†Ø¯Ú©Ø³
                results.append({
                    'rank': i + 1,
                    'chunk_id': idx,
                    'similarity_score': float(score),
                    'text': self.chunks[idx],
                    'metadata': self.chunk_metadata[idx]
                })
        
        print(f"âœ… {len(results)} Ù†ØªÛŒØ¬Ù‡ Ù…Ø±ØªØ¨Ø· ÛŒØ§ÙØª Ø´Ø¯")
        return results

    def save_system(self, filepath: str):
        """Ø°Ø®ÛŒØ±Ù‡ Ø³ÛŒØ³ØªÙ… RAG Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…Ø¬Ø¯Ø¯"""
        
        system_data = {
            'chunks': self.chunks,
            'embeddings': self.embeddings.tolist() if self.embeddings is not None else None,
            'chunk_metadata': self.chunk_metadata
        }
        
        with open(filepath, 'wb') as f:
            pickle.dump(system_data, f)
        
        # Ø°Ø®ÛŒØ±Ù‡ Ø§ÛŒÙ†Ø¯Ú©Ø³ FAISS
        if self.vector_index is not None:
            faiss.write_index(self.vector_index, filepath.replace('.pkl', '.faiss'))
        
        print(f"ğŸ’¾ Ø³ÛŒØ³ØªÙ… Ø¯Ø± {filepath} Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯")

    def load_system(self, filepath: str):
        """Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø³ÛŒØ³ØªÙ… RAG Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù‡"""
        
        with open(filepath, 'rb') as f:
            system_data = pickle.load(f)
        
        self.chunks = system_data['chunks']
        self.chunk_metadata = system_data['chunk_metadata']
        
        if system_data['embeddings']:
            self.embeddings = np.array(system_data['embeddings'])
            
        # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø§ÛŒÙ†Ø¯Ú©Ø³ FAISS
        faiss_path = filepath.replace('.pkl', '.faiss')
        try:
            self.vector_index = faiss.read_index(faiss_path)
            print(f"ğŸ“‚ Ø³ÛŒØ³ØªÙ… Ø§Ø² {filepath} Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯")
        except:
            print("âš ï¸ ÙØ§ÛŒÙ„ Ø§ÛŒÙ†Ø¯Ú©Ø³ ÛŒØ§ÙØª Ù†Ø´Ø¯ØŒ Ø§ÛŒÙ†Ø¯Ú©Ø³ Ù…Ø¬Ø¯Ø¯ Ø³Ø§Ø®ØªÙ‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯...")
            if self.embeddings is not None:
                self.build_vector_index(self.embeddings)